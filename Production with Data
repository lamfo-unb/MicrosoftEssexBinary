import time
start_time = time.time()
import os
import pyreadstat
from statsmodels.formula.api import ols
import pandas as pd
from collections import Counter
from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn import decomposition, ensemble
import pandas, xgboost, numpy, textblob, string
from keras.preprocessing import text, sequence
from keras import layers, models, optimizers
import pickle
import sys


os.chdir('C:\\Users\\Hiro Nagata\\Documents\\Python\\Scrapping')


data = pd.read_csv("articles.csv") #Poynter


data.drop(labels = {'Unnamed: 0', 'Explanation'}, axis = 1)

from collections import Counter

data = data[['Title', 'Label']]

data = data.dropna()

data = data.replace({'False': 'FALSE'}, regex=True)
data = data.replace({'Misleading': 'MISLEADING'}, regex=True)
data = data.replace({'Missing context': 'TRUE'}, regex=True)
data = data.replace({'half true': 'TRUE'}, regex=True)
data = data.replace({'PARTLY TRUE': 'TRUE'}, regex=True)
data = data.replace({'Unproven': 'FALSE'}, regex=True)
data = data.replace({'Correct': 'TRUE'}, regex=True)
data = data.replace({'Mostly FALSE': 'FALSE'}, regex=True)
data = data.replace({'MOSTLY FALSE': 'FALSE'}, regex=True)
data = data.replace({'Mainly false': 'FALSE'}, regex=True)
data = data.replace({'MOSTLY TRUE': 'TRUE'}, regex=True)
data = data.replace({"(Org. doesn't apply rating)": 'FALSE'}, regex=True)
data = data.replace({'MIsleading': 'MISLEADING'}, regex=True)
data = data.replace({'false': 'FALSE'}, regex=True)
data = data.replace({'mostly FALSE': 'FALSE'}, regex=True)
data = data.replace({"(FALSE)": 'FALSE'}, regex=True)
data = data.replace({'Fake': 'FALSE'}, regex=True)
data = data.replace({'misleading': 'MISLEADING'}, regex=True)
data = data.replace({'Partially FALSE': 'FALSE'}, regex=True)
data = data.replace({'Explanatory': 'TRUE'}, regex=True)
data = data.replace({'(FALSE)': 'FALSE'}, regex=True)
data = data.replace({'HALF TRUE': 'TRUE'}, regex=True)
data = data.replace({'Mostly True': 'TRUE'}, regex=True)
data = data.replace({'Two Pinocchios': 'FALSE'}, regex=True)
data = data.replace({'Half True': 'TRUE'}, regex=True)
data = data.replace({'Manipulated': 'FALSE'}, regex=True)
data = data.replace({'NO EVIDENCE': 'FALSE'}, regex=True)
data = data.replace({'Missing Context': 'TRUE'}, regex=True)
data = data.replace({'MISLEADING/FALSE': 'FALSE'}, regex=True)
data = data.replace({'Not true': 'FALSE'}, regex=True)
data = data.replace({'No Evidence': 'FALSE'}, regex=True)
data = data.replace({'True but': 'FALSE'}, regex=True)
data = data.replace({'partly FALSE': 'FALSE'}, regex=True)
data = data.replace({'Mixed': 'FALSE'}, regex=True)
data = data.replace({'Partially correct': 'FALSE'}, regex=True)
data = data.replace({'Misinformation / Conspiracy theory': 'FALSE'}, regex=True)
data = data.replace({'Needs Context': 'TRUE'}, regex=True)
data = data.replace({'IN DISPUTE': 'FALSE'}, regex=True)
data = data.replace({'FALSE headline': 'FALSE'}, regex=True)
data = data.replace({'Mostly true': 'TRUE'}, regex=True)
data = data.replace({'EXPLANATORY': 'TRUE'}, regex=True)
data = data.replace({'Unverified': 'FALSE'}, regex=True)
data = data.replace({'sarcastic': 'FALSE'}, regex=True)
data = data.replace({'HALF TRUTH': 'TRUE'}, regex=True)
data = data.replace({'Partially true': 'TRUE'}, regex=True)
data = data.replace({'Explicative': 'TRUE'}, regex=True)
data = data.replace({'Partially True': 'TRUE'}, regex=True)
data = data.replace({'Partly False': 'FALSE'}, regex=True)
data = data.replace({'Suspicious': 'FALSE'}, regex=True)
data = data.replace({'Partially true': 'TRUE'}, regex=True)
data = data.replace({'Explicative': 'Partially True'}, regex=True)
data = data.replace({'Out of Context': 'TRUE'}, regex=True)
data = data.replace({'FALSE and MISLEADING': 'FALSE'}, regex=True)


data = data.replace({'FALSE and MISLEADING': 'FALSE'}, regex=True)
data = data.replace({'Partly FALSE': 'FALSE'}, regex=True)
data = data.replace({'Suspicious': 'FALSE'}, regex=True)
data = data.replace({'FALSE news': 'FALSE'}, regex=True)
data = data.replace({'Unlikely': 'FALSE'}, regex=True)
data = data.replace({'mainly correct': 'FALSE'}, regex=True)
data = data.replace({'MANIPULATED': 'FALSE'}, regex=True)
data = data.replace({'Partly true': 'TRUE'}, regex=True)
data = data.replace({'missing context': 'FALSE'}, regex=True)
data = data.replace({'FALSO': 'FALSE'}, regex=True)
data = data.replace({'Suspicions': 'FALSE'}, regex=True)
data = data.replace({'MiSLEADING': 'MISLEADING'}, regex=True)
data = data.replace({'MiSLEADING': 'MISLEADING'}, regex=True)
data = data.replace({'partially FALSE': 'FALSE'}, regex=True)

data = data.replace({'Mostly FALSE': 'FALSE'}, regex=True)

data = data.replace({'partially FALSE': 'FALSE'}, regex=True)

data = data.replace({'MiSLEADING': 'MISLEADING'}, regex=True)
data = data.replace({'mislEADING': 'MISLEADING'}, regex=True)






#################################################
data = data.replace({'MISLEADING': 'FALSE'}, regex=True)
data = data.replace({'No evidence': 'FALSE'}, regex=True)
data = data.replace({'(FALSE)': 'FALSE'}, regex=True)

boolean_list = ['TRUE', 'FALSE']

data = data[data['Label'].isin(boolean_list)]
#################################################




guardian = pd.read_csv("Guardian.csv") #Guardian

test = guardian[:11761]
test = pd.DataFrame(test)

test['Label'] = 'TRUE'
test.columns = ['Title', 'Label']

WP = pd.read_csv("WashingtonPost.csv")  #Washington Post
CNN = pd.read_csv("CNN.csv") #CNN

WP = WP[['Title']]
CNN = CNN[['Title']]


WP = WP.dropna()
CNN = CNN.dropna()

WP['Label'] = 'TRUE'
CNN['Label'] = 'TRUE'

test = pd.concat([test, WP])
test = pd.concat([test, CNN])

data = pd.concat([data, test])


data = data[~data['Title'].str.contains("Guardian")]


import csv

with open('covid-news.csv', 'w', newline='', encoding='utf-8') as csv_file:
    writer = csv.writer(csv_file, delimiter=';')
    writer.writerow('my_utf8_string')

data.to_csv('covid-news.csv', encoding = "utf-8", index = False)


data3 = pd.read_csv("covid-news.csv") 

def train_binary():
    # os.chdir('C:\\Users\\Hiro Nagata\\Documents\\Python\\Scrapping') #TODO write relative paths

    data3 = pd.read_csv("covid-news.csv") #TODO change to the full processing algo
    data = data3

    # 1. Dataset preparation
    # load the dataset

    a = data['Title'].tolist()
    b = data['Label'].tolist()

    labels, texts = [], []

    for i in range(len(a)):
        content = a[i].split()
        #content = line.split()
        lab = b[i]
        labels.append(lab)
        texts.append(" ".join(content))

    # create a dataframe using texts and lables
    trainDF = pd.DataFrame()
    trainDF['text'] = texts
    trainDF['label'] = labels


    # 2. Feature Engineering

    # split the dataset into training and validation datasets 
    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])

    # label encode the target variable 
    encoder = preprocessing.LabelEncoder()
    train_y = encoder.fit_transform(train_y)
    valid_y = encoder.fit_transform(valid_y)

    # 2.1 Count Vectors as features

    # create a count vectorizer object 
    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}')
    count_vect.fit(trainDF['text'])

    # transform the training and validation data using count vectorizer object
    xtrain_count =  count_vect.transform(train_x)
    xvalid_count =  count_vect.transform(valid_x)


    # 3. Model Building

    results = []

    def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):
        # fit the training dataset on the classifier
        classifier.fit(feature_vector_train, label)
        
        # predict the labels on validation dataset
        predictions = classifier.predict(feature_vector_valid)
        
        if is_neural_net:
            predictions = predictions.argmax(axis=-1)
        
        return metrics.accuracy_score(predictions, valid_y), classifier

    def train_model2(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):
        # fit the training dataset on the classifier
        classifier.fit(feature_vector_train, label)
        
        # predict the labels on validation dataset
        predictions = classifier.predict(feature_vector_valid)
        
        if is_neural_net:
            predictions = predictions.argmax(axis=-1)
        
        return predictions





    # 3.1 Naive Bayes

    # Naive Bayes on Count Vectors
    accuracy, classificador = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)
    print("NB, Count Vectors: ", accuracy)
    a = "NB, Count Vectors: ", accuracy
    results.append(a)


    #Export all the trained models!

    with open('classificador.pickle', 'wb') as handle:
        pickle.dump(classificador, handle)

    with open('xtrain_count.pickle', 'wb') as handle:
        pickle.dump(xtrain_count, handle)

    with open('train_y.pickle', 'wb') as handle:
        pickle.dump(train_y, handle)

    with open('count_vect.pickle', 'wb') as handle:
        pickle.dump(count_vect, handle)

    



###############################################################################################################################

# CLASSIFICATION

def classify(new_data):

    os.chdir('../MicrosoftEssexBinary/')

    with open('classificador.pickle', 'rb') as handle:
        classificador = pickle.load(handle)

    with open('xtrain_count.pickle', 'rb') as handle:
        xtrain_count = pickle.load( handle)

    with open('train_y.pickle', 'rb') as handle:
        train_y = pickle.load( handle)

    with open('count_vect.pickle', 'rb') as handle:
        count_vect = pickle.load( handle)

    
        
        
    #Classify here with the imported pickle
    texts = []

    texts.append(new_data)


    trainDF2 = pd.DataFrame()

    trainDF2['text'] = texts

    data2 = trainDF2.squeeze()
    data2 =  pd.Series(data2)
    xvalid_count2 = count_vect.transform(data2)

    # 3. Model Building
    results2 = []


    # Naive Bayes on Count Vectors
    # b = train_model2(classificador, xtrain_count, train_y, xvalid_count2)

    classificador.fit(xtrain_count, train_y)
    # predict the labels on validation dataset
    predictions = classificador.predict(xvalid_count2)
    b = predictions
    
    b = "NB, Count Vectors: ", b
    results2.append(b)

    return results2[0][1][0]


    #################################################################
# train_binary()
# new_data = 'Pfizer vaccine are causing troublesome effects on grannies'
# new_data = 'Pfizer vaccine saves lifes'
# result = classify(new_data)
# print(int(result))
# print("--- %s seconds ---" % (time.time() - start_time))
